# 2.9 相关性搜索（具有上下文的老虎机）

到目前为止，在本章中，我们只考虑了非关联性任务，即不需要考虑不同情况下不同行为的任务。在这些任务中，学习者要么在任务静止时试图找到一个最好的动作，要么随着时间的变化来追踪最佳的动作。然而，在一般的强化学习任务中，不止一种情况，目标是学习一个策略：从情况到那些情况下最好的行动的映射。为了为整个问题搭建舞台，我们简要讨论非关联任务扩展到关联设置的最简单方法。

例如，假设有几个不同的多臂老虎机任务，而且每一步你都要随机选择一个。因此，老虎机任务每一步都会随机地变化。这看起来像是一个单一的，非平稳多臂老虎机任务，其真实行为价值每一步都随机进行变化。你可以尝试使用本章描述的可以处理非平稳性的方法，但除非真实行为价值变化缓慢，否则这些方法将不能很好地工作。现在假设，当你选择一个土匪任务时，你会得到一些关于其身份的独特的线索（但不是它的行为价值）。现在假设，当你选择一个老虎机任务时，你会得到一些关于其身份的独特的线索（但不是它的行为价值）。也许你正面临着一台真正的老虎机，它会根据它的行为价值改变显示颜色。现在你可以学习一个关联每个任务的策略，用你所看到的颜色来表示每个任务，并且在面对这个任务时采取最好的行动——例如，如果是红色的，选择手臂1；如果绿色，选择手臂2。这样你会比你缺少能区分老虎机任务的信息时做的更好。

这是一个相关性搜索的例子，叫这个名字，是因为它涉及到试错学习，以寻求最佳的行动，并把这些最佳行动和对应的情况联系起来。相关性搜索在文献中通常被称为情境老虎机。它介于多臂老虎机问题和完全强化学习问题之间。他们就像完全的强化学习，因为他们涉及到学习策略，也像我们的多臂老虎机问题，因为每个行为只影响即刻的奖励。如果行为会影响到其他的情境和奖励，那么这个问题就变成了完全的强化学习问题。我们将在下一章中介绍这个问题，并在本书的其余部分考虑它的含义。

练习2.8 假设你面对一个两臂老虎机问题，其真实行为价值随着时间步骤随机变化。具体来说，假设对于任何的时间步骤，动作1和2的真实值分别为0.1和0.2，概率为0.5（情境A），以及0.9和0.8，概率为0.5（情境B）。如果你无法确定你在哪个情境，那么你能取得的最佳期望是什么呢，你应该如何实现呢？现在假设在每一步你都被告知你是面对情境A或情境B（尽管你仍然不知道真实的行动价值）。那么你能取得的最佳期望是什么呢，你应该如何实现呢？