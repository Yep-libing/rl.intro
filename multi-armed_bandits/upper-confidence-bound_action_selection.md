# 2.7 置信上界法

我们需要进行“探索”是因为“行为价值”（action-value）的估计总是不准确的。贪婪行为在当时可能是最好的选择，但是其他的行为实际上可能表现地更好。$\varepsilon$-贪婪行为选择强制尝试那些非贪婪行为，但是对这些非贪婪行为，不管是近似贪婪或者特别不确定的行为，并没有偏好。更好的做法是考虑这些估计的最大值和不确定性，根据他们成为最佳行为的潜力来选择这些非贪婪行为。一种有效的选择方法是根据下面的式子进行选择：

$$
A_t  \doteq \arg\max_a[Q_t(a)+c\sqrt{\cfrac{\ln{t}}{N_t(a)}}]
$$

$\ln{t}$表示$t$的自然对数，$N_t(a)$表示行为$a$在时间$t$之前被选择的次数（公式（2.1）的分母），数字c>0控制探索的程度。如果$N_t(a)=0$，那么a被看做是最大化行为。

置信上届法行为选择的思路是平方根项是对a的价值估计的不确定性或者方差的度量。因此，最大化的数量是行动a的可能真值的一个上限，c决定了置信水平。每次行为a被选择，a的不确定性会降低：$N_t(a)$变大，因为它在分母，所以不确定项减小。另一方面，每当选择a以外的动作时，t增加，但是$N_t(a)$不增加；由于t出现在分子中，所以不确定性估计值增加。使用自然对数的原因是不确定性应增长随着时间的推移变小，但是应该是无限的；所有的行动将最终被选中，但是价值较低的行动或者已经被频繁选择的行动将随着时间的推移而逐渐减少被选中。

如图2.4，显示了在多臂测试平台上使用ucb的结果。如此处所示，ucb通常表现良好，但是比$\varepsilon$-贪婪更难拓展到更通用的强化学习中。一个困难是处理非平稳问题；这就需要比第2.5节中提出的方法更为复杂的方法了。另一个困难是处理大的状态空间，特别是当使用本书第二部分所介绍的函数逼近时。在这些更高级的环境中，ucb动作选择的想法通常是不实际的。