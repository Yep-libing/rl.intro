# 1.7 增强学习的早期历史
增强学习的早期历史中存在两条路线，都有着悠久而丰富的历史，两者一直独立发展直到现代增强学习将他们合二为一。其中一条路线研究通过试错来学习，这一想法来源于心理学中动物学习的启发。这一路线一直贯穿于人工智能研究的早期，并且带来了1980年代增强学习的复兴。另外的一条路线考虑最优控制的问题以及如何使用价值函数和动态规划来解决问题。这一路线大部分情况下不涉及学习。这两条路线都互相独立区别明显，但是第三个关注时间差分方法的路线，则相比前两者没有那么显著的区别，在本章的井字棋中使用的就是这一方法。这三条路线最终在1980年代后期融合在一起产生了现代增强学习这一领域，本书就是介绍这一领域的。

专注于试错学习的路线我们最熟悉的，也是这一简史中大量篇幅所在。在我们讨论这条线路之前，先简要讨论下最优控制路线。

“最优控制”这个词语在1950年代后期开始使用，用来描述如何设计控制器在时间维度上最小化动态系统行为的某一个指标的问题。其中的一个方法在1950年代中期由Richard Bellman和其他研究人员一起开发出来，用来扩展19世纪时Hamilton和Jacobi的理论。这一方法利用动态系统状态和价值函数（或者称作“最优回报函数”）两个概念来定义一个函数方程，现在这个方程一般被叫做Bellman方程。通过这个方程解决这类最优控制问题的各种方法就叫做动态规划（Bellman, 1957a）。Bellman（1957a）还介绍了离散概率版本的最优控制问题，这一问题被成为马尔可夫决策过程（MDPs）,Ronald Howard (1960)发明了用于MDPs的策略迭代法。而以上所说的内容都是现代增强学习的理论和各种算法的基础元素。

动态规划被广泛的认为解决通用随机最优控制的唯一可行方法。虽然这一方法受到Bellman说的“维度灾难”所困扰，“维度灾难”造成计算资源需求随着状态变量个数的增加指数级增加，动态规划依然相比其他通用方法更加有效、更具有可行性。动态规划已经被广泛的开发，包括扩展用于处理部分可观测的MDPs(Lovejoy, 1991)，各种应用（White, 1985, 1988, 1993），逼近方法（Rust, 1996），还有异步方法（Bertsekas, 1982, 1983）。目前已经有很多非常好的动态规划的方案(比如Bertsekas, 2005, 2012; Puterman, 1994; Ross, 1983; Whittle, 1982, 1983)。Bryson (1996)提供了最优控制的比较权威的历史介绍。

最优控制与动态规划的联系，或者说与学习的关系，是慢慢被发现的。我们不知道两者没有联系在一起的准确原因，但是主要原因可能是他们涉及不同学科领域，这些领域的希望达到的目标是不同的。也有可能还因为大家普遍认为动态规划是离线计算，特别依赖准确的系统模型和Bellman方程的解析解。还有就是动态规划的最简单形式是在时间上逆向的计算，这个很难让人想到如何将这个方法应用于学习过程，而学习过程必须是在时间上正向进行的。最早的动态规划的工作，比如说Bellman和Dreyfus(1959) 的，现在可以认为是遵循学习方法的路线。Witten (1977) 的工作 (下文会讨论) 可以很肯定的认为是学习和动态规划两种想法的结合。Werbos (1987)明确的提出了动态规划与学习方法的相互关系，以及与理解神经和认知模式的关系。我们已知直到Chris Watkins在1989年的工作才有了动态规划方法与在线学习的完全整合，他使用MDP的形式来解决增强学习问题的方法(Watkins, 1989)已经被广泛的应用了。在此之后这些关系就开始被大量的研究人员广泛扩展，尤其是Dimitri Bertsekas和John Tsitsiklis (1996)的工作，他们提出了“神经动态规划”，结合动态规划与神经网络。还有一个现在在用的名称就是“近似动态规划（approximate dynamic programming）”。这些方法着重于这个问题的各个不同方面，但是这些方法都可以利用增强学习来可以克服动态规划的缺点。

我们可以一定程度上认为所有最优控制相关的工作也是增强学习领域内的。增强学习方法是任何一个可以有效解决增强学习问题的手段，现在已经很明确的知道这些问题与最优控制问题都有密切的联系，特别是那些被表示为MDPs的随机最优控制问题。对应的我们就必须认为最优控制的解决方法，比如说动态规划，也是增强学习方法。由于几乎所有的传统方法都需要能够控制系统的所有信息，所以说这些方法是增强*学习*有点显得不自然。另外，很多动态规划的方法是增强和迭代的。就像学习方法一样，他们通过不断的逼近来逐渐的获得准确结果。如本书后面所展示的，这些相似点不仅仅是表面的。用来解决完备知识与不完备知识案例的理论和解决方法十分相近，以至于我们会觉得他们肯定是共同构成相同主题的一部分。


我们回到另外的一个最终引发现代增强学习领域的路线上，这一路线着重于试错学习这一想法。我们现在只简要介绍这个问题的一些要点，第14章会更加详细的介绍这个主题。根据美国心理学家 R. S. Woodworth的说法，试错学习的想法可以一直追溯到1850年代Alexander Bain有关通过“摸索和试验”来学习的讨论，更加明确的提出则是英国行为学家、心理学家Conway Lloyd Morgan在1894年使用这一词语表述他对动物行为的观察(Woodworth, 1938)。可能第一个简要表达试错学习方法本质是学习的原理之一的人是Edward Thorndike：

    针对同一情景的不同反应中，其他条件相同的话，那些伴随、或者紧随动物满足感的反应，会更加与情景紧密联系，以便这一情景出现时，这些反应也更加容易出现。其他条件相同的情况下，那些伴随或者紧随动物的不适感的反应，会与情景的联系更弱，这样这一情景出现时，这些反应就更不容易出现。满意度或不适感越大，这一联系的加强或减弱就越大。(Thorndike, 1911, 244页)

Thorndike把以上内容称作是“有效法则”，因为其描述了强化事件对选择行为倾向的影响。Thorndike后来完善了这一法则以更好的适用动物学习研究的积累数据（比如奖励和惩罚效果的不同点），它的各种形式在学习理论家中引起了相当大的争论(比如可参考Gallistel,2005; Herrnstein, 1970; Kimble, 1961, 1967; Mazur, 1994)。尽管如此，效应定律（以某种形式）还是被广泛认为是各种动物行为背后的基础原理（例如，Hilgard和Bower，1975; Dennett，1978; Campbell，1960; Cziko，1995）。 它是Clark Hull有影响力的学习理论和B.F. Skinner的实验方法（例如Hull，1943; Skinner，1938）的基础。


在描述动物学习中使用的“强化（reinforcement）”一词是在Thorndike的效果法则中出现的表述，据我们所知，该词在该方面的第一次出现是在1927年发行的Pavlov关于条件反射的专著的英文译本中。所谓强化是指在动物受到刺激——强化物（reinforcer）——后对某种行为模式的加强，该刺激与另一种刺激或反应具有一定的时态关系。一些心理学家将“强化”一词的含义从单纯的强化拓展到削弱，并将其应用于忽略某事件或某事件终止对动作产生影响的情况中。强化过程将对行为产生持久改变，即便发起强化的强化物已经消失，所以如果一个刺激吸引了动物的注意并刺激了某种行为，但并未对动物行为产生持久性的改变，那么这种刺激就不被认为是强化物。

在计算机中应用试错学习的想法是在最早思考人工智能的可能性的时候就产生了的，在1948年的一篇报告中，Alan Turing描述了一种按照效果法则进行工作的“快乐-痛苦系统”：

         当一个配置中出现不确定的动作时，由于缺少数据，应采取一种随机的选择同时构造并采用一
         个描述性的临时的动作入口，如果因为该动作产生了痛苦刺激，那么所有临时入口被取消，如
         果因为该动作产生了快乐刺激，那么所有临时入口被转变为长期的。（Turing，1948）

许多精巧的机电机器的构建都体现了试错学习的思想。最早的这样的机器或许是Thomas Ross (1933)研发的，这个机器可以在一个简单迷宫中寻路并且记住通过交叉口的路。1951年，已经以“机械乌龟”（Walter，1950）闻名的W. Grey Walter，又搭建了一个能够实现简单学习的机器版本（Walter，1951）。1952年Claude Shannon展示了一只名为Theseus的老鼠，这只老鼠可以通过试错找到通过迷宫的路，而迷宫本身可以通过地板下的继电器和磁铁记录下成功通过的路径（Shannon，1951,1952）。1954年J. A. Deutsch提出了一种基于他自己行为理论（Deutsch，1953）的解决迷宫问题的机器，该机器与基于模型的强化学习方法（第8章）有一些相似性。1954年，Marvin Minsky在他的博士毕业论文中讨论了强化学习的计算模型，并描述称他构建的模拟机中包含有被他称为SNARCs（随机神经模拟增强计算器Stochastic Neural-Analog Reinforcement Calculators）的部分，该部分主要为了类比大脑中可修改的突出连接（第15章）。cyberneticzoo.com网站上有大量的类似这些机器的机电学习机的信息。

随着发展，构造机电学习机逐渐被编写数字计算机程序所取代，这些程序可以实现多种学习过程，其中一些应用了试错学习的方法。1954年Farley和Clark进行了一些数字模拟，主要针对利用试错进行学习的神经网络学习机，但之后他们的兴趣就转向了泛化和模式识别，也就是从强化学习转向了监督学习（Clark和Farley，1955），这个例子正描绘了当时很多学者对不同学习类型之间的关系难以区分的困囧局面，很多学者认为他们在从事强化学习研究，但实际上他们研究的却是监督学习。比如，神经网络领域的先驱Rosenblatt (1962)、 Widrow 和 Hoff (1960)都很明显是受到了强化学习的激励，因为他们使用类似奖励和惩罚这样的术语，但他们研究的却是适用于模式识别和知觉学习的监督学习系统。直到今天，还有一些学者或参考书会最小化或模糊不同学习类型的区别，比如，一些神经网络的参考书使用“试错”来描述网络训练的过程，这是一个可以理解的误区，因为这些网络的确使用误差信息来更新权重，但这种使用误解了试错学习的本质——通过可评估的反馈来选择动作，而不依靠对于正确动作的理解。
​    
这种对于学习类型区分的困境，正是导致1960和1970年代真正的试错学习变得很稀少的一个原因，尽管仍有一些值得瞩目的例外。在20世纪60年代，“强化”和“强化学习”这些术语第一次被用于工程文学，用来描述试错学习（如Waltz和Fu，1965；Mendel，1966；Fu，1970；Mendel和McClaren，1970），其中由Minsky撰写的题为《Steps Toward Artificial Intelligence》的论文产生了重大的影响，在这篇论文中讨论了一些与试错学习相关的问题，包括预测，期望以及被他称为复杂强化学习系统的基本信用分配问题（basic credit-assignment problem for complex reinforcement learning systems）：如何给实现成功的过程中所做出的决定分配信用值？本书所讨论的所有方法，从某种程度上讲都是在解决这个问题，Minsky的论文直到今天依然非常值得阅读。
​    
尽管20世纪60,70年代对于试错学习的计算和理论研究相对容易被忽视，但其中仍然有一些例外，下面几个段落我们将对这些例子进行讨论。
​    
这其中包括一位名叫John Andreae的新西兰研究人员的研究，Andreae在1963年开发了这一种名为STeLLA的系统，该系统可以在与环境互动的过程中进行试错学习，这个系统内部包含有关于世界的模型，并在之后又发展出了可以处理隐藏状态的“内心独白”（Andreae，1969a），后续研究中Andreae（1977）将工作重点放在了从优秀经验中学习，但系统仍然会通过试错来学习，旨在产生新颖的未知的结果或状态。STeLLA系统的一个特征就是“回漏过程”，Andreae在1998年将这种过程进行了更加精妙的设计，采用了一种信用分配机制，这种机制与我们之前讨论的回溯更新操作相似。不幸的是，尽管Andreae的工作是先驱性的，但却并不著名，也未对后续的强化学习研究产生很大的影响。
​    
在同期的研究工作中，Donald Michie的研究较Andreae有更大的影响。Donald Michie在1961年和1963年研发了一种简单的试错学习系统用来学习下井字棋，该系统被称为MENACE（Matchbox Educable Naughts and Crosses Engine可学习井字棋盒子工具）。MENACE系统的工作机制是，首先为每个可能的游戏位置创建一个盒子，每个盒子中包含有一定量的彩色珠子，不同颜色代表该位置的不同可能动作，而后在游戏过程中通过从对应当前位置的盒子中拿出一个珠子，MENACE可以根据珠子颜色决定下一步动作，最后当一句游戏结束后，通过向游戏中使用过的盒子增减珠子来强化或惩罚MENACE的决策。1968年，Michie和Chambers研发了另一种关于井字棋的名为GLEE（Game Learning Expectimaxing Engine游戏学习专用引擎）的强化学习器，和一个名为BOXES的强化学习控制器。他们使用BOXES来完成对于铰连接在可移动小车上的杆子的平衡控制（倒立摆），当杆子失去平衡或小车超出可移动范围则任务失败，该控制任务是从Widrow和Smith在1964年的工作中借鉴而来的，Widrow二人在解决倒立摆任务时使用了监督学习方法，过程中假设系统具备可供学习的倒立摆成功控制经验。Michie和Chambers的杆平衡任务解决方案，是早期用来说明强化学习可以解决知识不完备任务的最好例子之一，它影响了很多后续的的强化学习研究工作，包括一些我们自己的研究（Barto，Sutton和Anderson，1983；Sutton，1984）。后来，Michie也一直强调试错和学习是人工智能的本质（Michie，1974）。

Widrow，Gupta和Maitra在1973年修改了Widrow和Hoff在1960年提出最小均方算法（LMS），他们通过修改LMS，使强化学习获得了从失败或成功的结果中学习的规则，而不必通过训练样本学习，他们将这种学习形式称为“选择性自举适应（selective bootstrap adaptation）”，将其形容为一种“带评论家的学习”而非“带指导者的学习”。Widrow等人分析了这种规则，并展示了强化学习算法如何通过这种新规则学习玩“二十一点”扑克游戏，Widrow的这项研究是他对强化学习领域为数不多的一个尝试，他对于监督学习的影响要远远大于其对强化学习的影响，今天我们使用的“评论家（critic）”一词就是来源于Widrow，Gupta和Maitra的论文。1978年Buchanan, Mitchell, Smith和Johnson在机器学习研究中各自独立的使用了评论家这一术语（Dietterich和Buchanan在1984年也使用了该词），但是对于他们来说，评论家代表着一个专家系统，该系统可以做的不仅仅是对表现进行评估。

对学习自动机（learning automata）的研究，对于强化学习试错方法分支有着非常直接的影响，也揭开了现代强化学习研究的篇章。这些研究所针对的都是一种非相关的，单纯的选择学习问题，该问题一般称为k臂赌博机问题（k-armed bandit），或没有k个拉杆的“独臂老虎机”问题（第2章）。学习自动机是一种为了提高在这类问题中获利概率的简单低速存储机器，最早源于俄罗斯数学家，物理学家M. L. Tsetlin和他的同事在20世纪60年代的研究（发表于1973年，Tsetlin逝世后），自发表后学习自动机在工程领域得到了巨大的发展（Narendra和Thathachar，1974,1989），这些发展包括对于随机学习自动机的研究，该学习自动机主要用于给予奖励信号的动作概率更新。随机学习自动机出现前，一些早期的心理学研究工作为其奠定了基础，包括1950年始于William Estes并被其他学者进一步研究的学习随机理论，该理论的发展中最著名的为心理学家Robert Bush和统计学家Frederick Mosteller在1955年的研究。

统计学习理论最初发源于心理学，而后被经济学学者采纳用于经济学研究，后期又成为强化学习的一个研究分支。关于经济学的这项研究开始于1973年，最初是将Bush和Mosteller的学习理论应用于一些经典经济学模型（Cross,1973)，该研究的一个目标就是研究比传统理想经济主体更像人类的人工主体（Arthur，1991），后来在博弈论的背景下关于统计学习的研究拓展到了强化学习领域。尽管早期经济学领域的强化学习发展独立于人工智能领域的研究工作，但当前强化学习和博弈论成为了两个领域学者都关心的话题，虽然本书并不讨论博弈论。Camerer（2003）讨论介绍了强化学习在经济学中的历史沿革，Nowé等人（2012）概述了从多代理角度对于强化学习的理解，这些讨论对于本书内容做出了补充。相比研究如何使用强化学习编程解决井字棋，国际跳棋以及其他有趣的游戏问题来讲（2012年Szita在这方面进行了一个关于强化学习和游戏的概述），博弈论背景下的强化学习是一个非常不同的研究领域。

1975年John Holland概述了基于选择原则的自适应系统的一般理论，他的早期研究主要针对非相关形式的试错方法，就如同在进化方法和k臂赌博机中出现的那样，而后在1976年（1986年得到了进一步充实），John Holland研发了分类器系统，该系统是一种考虑了相关性和价值函数的真正的强化学习系统。这个分类器系统的一个核心部分就是用于信用分配的“桶队算法（bucket-brigade algorithm）”，该算法与我们在井字棋例子中使用的时序差分算法有密切的关系，我们将在第6章加以讨论；另一个核心部分是遗传算法（genetic algorithm），该算法是一种进化方法，主要起到发展有益部分的作用。自诞生以来，分类器系统得到了众多学者的研究与发展，现今已经成为了强化学习研究领域的一个主要分支（Urbanowicz和Moore在2009年进行了总结概述），但是遗传算法——我们并不将其本身作为强化学习系统——获得了更多的关注，就像其他进化计算方法一样（如Fogel，Owens和Walsh，1966，Koza，1992）。

在人工智能领域，对增强学习的试错学习分支最有影响力的就是Harry Klopf (1972,  1975,  1982)。Klopf认识到，因为学习研究者几乎完全专注于监督学习，对自适应行为的研究越来越少。根据Klopf的观点，我们缺少的对行为的研究，自适应行为从环境中获得某种结果，控制环境朝着期望的目标前进，远离不希望的目标。这是试错学习的基本思想。Klopf的观点对我们而言尤其重要，因为我们对这些观点的评估（Barto and Sutton，1981a）使我们认识到监督和强化学习之间的区别，并且使我们最终关注强化学习。我们和同事所完成的大部分早期工作都指出强化学习和监督学习确实是不同的（Barto,Sutton, and Brouwer, 1981; Barto and Sutton, 1981b; Barto and Anandan, 1985）。其他研究表明强化学习可以解决神经网络学习中的重要问题，特别是如何为多层网络生成学习算法（Barto, Anderson, and Sutton, 1982;Barto and Anderson, 1985; Barto and Anandan, 1985; Barto, 1985, 1986; Barto and Jordan, 1987）。在第15章中，我们会详细介绍强化学习和神经网络。

现在我们来看看关于时间差异学习的强化学习分支的历史。时间差分学习方法由相同数量的时间连续估计之间的差异所驱动，例如在井字游戏示例中获胜的概率的差异是有区别的。这个分支比其他两个分支更小，更不起眼，但在这个领域中起到了特别重要的作用，部分原因在于时间差分方法是全新的、独特的。

时间差分学习部分起源于动物学习心理学中，比如辅助增强剂的概念。辅助强化剂是与食品或疼痛等主要强化剂配对的刺激物，因此具有类似的强化特性。Minsky（1954）也许是第一个认识到这种心理学原理对于人工学习系统是重要的。Arthur Samuel （1959）是第一个提出并实施一种包括时间差分思想的学习方法，作为他著名的跳棋程序的一部分。

Samuel没有提及 Minsky的工作，也没有提到可能与动物学习的关系。他的灵感显然来自于Claude Shannon（1950）的一个建议，即计算机可以编程来使用评估函数下棋，也可以通过在线修改这个函数来改善它的表现。（Shannon的这些想法也可能影响了Bellman，但是我们没有这方面的证据。）Minsky（1961）在他的“Steps”文章中广泛地讨论了Samuel的工作，提出了与天然和人造的二级强化理论的联系。

正如我们已经讨论过的，在Minsky和Samuel之后的十年里，在试错学习方面做了很少的计算工作，显然在时间差分学习上没有任何计算工作。1972年，Klopf为试错学习带来了一个重要部分，时间差分学习。Klopf对可以在广泛的系统中进行规模学习的原则感兴趣，他被本地强化的概念所吸引，即整个学习系统的子组件可以相互强化。他提出了“广义强化”的概念，每个部分（名义上指每个神经元）都以强化的形式来看待它的所有输入：兴奋性输入作为奖励，抑制性输入作为惩罚。这与我们现在所知道的时间差分学习不是一回事，回想起来，它比Samuel的工作走得更远。另一方面，Klopf将这个想法与试错学习联系起来，并将其与大量的动物学习心理学实证数据相联系。

Sutton（1978a，1978b，1978c）进一步发展了Klopf的观点，尤其是与动物学习理论的联系，描述了由时间上连续的预测变化所驱动的学习规则。他和Bartore提炼了这些想法，并开发了一个基于时间差分学习的经典调节心理模型（Sutton and Barto，1981a; Barto and Sutton，1982）。接下来还有其他一些基于时间差分学习的经典调节的有影响力的心理模型（e.g.,Klopf, 1988; Moore et al., 1986; Sutton and Barto, 1987, 1990）。此时开发的一些神经科学模型在时间差分学习方面得到很好的解释（Hawkins and Kandel, 1984;Byrne, Gingrich, and Baxter, 1990; Gelperin, Hop eld, and Tank, 1985; Tesauro, 1986; Friston et al.,1994）,尽管在大多数情况下没有历史联系。

我们在时间差异学习方面的早期工作受到动物学习理论和Klopf工作的强烈影响。与Minsky的“Steps”论文和塞缪尔的跳棋选手之间的关系只在之后才被认识到。但到了1981年，我们完全意识到上述所有的前期工作是时间差分和试错学习的一部分。在这个时候，我们开发了一种使用时间差分学习和试错学习相结合的方法，称为演员 - 评论家架构，并将这种方法应用于Michie和Chambers的倒立摆平衡问题（Barto, Sutton, and Anderson, 1983）。这种方法在Sutton（1984）博士论文被广泛研究。并在Anderson（1986）博士论文中被扩展使用反向传播神经网络。大约在这个时候，Holland（1986）将时态差分思想以其桶形旅行算法的形式明确地纳入了他的分类系统。Sutton于1988年采取了一个关键步骤，将时差学习与控制分离，并将其作为一般预测方法。该论文还介绍了TD（λ）算法，并证明了它的一些收敛性质。

当我们在1981年完成我们关于演员 - 评论家架构的工作时，我们发现了Ian Witten（1977）的一篇论文，这篇论文似乎是时间差分学习规则的最早出版物。他提出了现在称为表格TD（0）的方法，作为解决MDP的自适应控制器的一部分。Witten的工作是Andreae在STeLLA和其他试错学习系统的早期实验的后代。因此，Witten在1977年的论文跨越了强化学习研究的两大主线——试错学习和最优控制——同时也对时态差分学习做出了独特的早期贡献。

1989年，Chris Watkins发展了Q学习，时序差分和最优控制分支被完整地结合起来。这项工作扩展和综合了所有三个强化学习研究分支的工作。Paul Werbos（1987）自1977年以来一直在促进试错学习和动态规划的融合。在Watkins's工作的时候，强化学习研究有了巨大的发展，主要是在人工智能的机器学习子领域，和更广泛的神经网络领域。在1992年，Gerry Tesauro的双陆棋游戏程序和TD-Gammon的显著成功给该领域带来了更多的关注。

自从本书第一版出版以来，神经科学的一个子领域蓬勃发展，专注于强化学习算法与神经系统强化学习之间的关系。对此，最显著的是时间差分算法的行为与大脑中产生多巴胺的神经元的活性之间神秘的相似性，正如一些研究人员所指出的那样（(Friston et al., 1994; Barto, 1995a; Houk, Adams, and Barto,1995; Montague, Dayan, and Sejnowski, 1996; and Schultz, Dayan, and Montague, 1997）第15章介绍了强化学习的这个令人兴奋的方面。

在最近的强化学习历史中，其他重要的贡献在这个简短的叙述中不胜枚举。我们在其出现的各个章节的最后引用了其中的许多内容。

# 书目备注

关于强化学习的其他一般性内容，我们建议读者阅读zepesvari(2010), Bertsekas and Tsitsiklis (1996), Kaelbling (1993a), and Sugiyama et al. (2013)等人的着作。控制或操作研究视角的书籍包括 Si et al. (2004), Powell (2011), Lewis and Liu (2012), and Bertsekas (2012)等人的书籍。Cao（2009）的评论将增强学习放置在其他学习方法和优化随机动态系统的背景下。“机器学习”杂志侧重于强化学习的三个特殊问题：Sutton（1992），Kaelbling（1996）和Singh（2002）。由 Barto (1995b);  Kaelbling, Littman, and Moore (1996);and Keerthi and Ravindran (1997)提供的有用的调查。由Weiring和van Otterlo（2012）编辑的书目为最近的发展提供了一个很好的概述。

**1.2** 在本章中，菲尔早餐的例子受到了Agre（1988）的启发。

**1.5** 第6章介绍了在井字示例中使用时间差分的方法。